import google.generativeai as genai
from vertexai.preview import tokenization
from typing import List
from configs import *


class SummarizationAgent():
    def __init__(self,system_prompt: str ="Your task is to do Extractive Summarization of the user input",temperature: float = 0.4, max_output_tokens: int = 1000):
        self.MAX_SIZE = config()
        self.system_prompt = system_prompt
        self.model = genai.GenerativeModel(
                model_name=GEMINI_MODEL,
                system_instruction=self.system_prompt)
        
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens
        self.tokenizer = tokenization.get_tokenizer_for_model(GEMINI_MODEL)

    def generate_response(self, prompt: str) -> str:
        response = self.model.generate_content(
                prompt,
                generation_config = genai.GenerationConfig(
                    max_output_tokens=self.max_output_tokens,
                    temperature=self.temperature))

        r_text = response.text
        return r_text 

    def map(self,prompt:str) -> List[str]:
        size = self.model.count_tokens(prompt)

        chuncks = [""]
        while(prompt):
            chuncks[-1] += prompt[:self.MAX_SIZE] 
            prompt = prompt[self.MAX_SIZE:]
            if(self.tokenizer.count_tokens(chuncks[-1]).total_tokens >= self.MAX_SIZE):
                chuncks.append("")

        responses = [self.generate_response("<TEXT>"+c) for c in chuncks]
        return responses
    
    def reduce(self,responses: List[str]) -> str:
        concat_r = "<JSON>" + "\n".join(responses)
        return self.generate_response(concat_r)

    def summarize(self,prompt: str) -> str:
        return self.reduce(self.map(prompt))

with open("sys_prompts/extractive_summarization.txt") as f:
    system_prompt = f.read()

m = SummarizationAgent(system_prompt=system_prompt)

p = """
Okay, now let’s come to the abstractive summarization method. The name implies that it has arrived from the root form of the word abstract, which means outline/summary or the basic idea of a voluminous thing (text). Unlike the extractive summarization models, it simply doesn’t pick out the important sentences. Rather, it analyses the input text and generates new phrases or sentences that capture the essence of the original text and convey the same meaning as the original text but more concisely and coherently.

Again, how exactly is the summary generated using this method? In brief, the input text is analyzed by a neural network model that learns to generate new phrases and sentences that capture the essence of the original text. The model is trained on large amounts of text data and learns to understand the relationships between words and sentences, generating new text that conveys the same meaning as the original text in a more understandable manner.

This method uses advanced NLP techniques such as natural language generation (NLG) and deep learning to understand the context and generate the summary. The resulting summaries are usually shorter and more readable than the ones generated by the extractive summarization models, but they can sometimes contain errors or inaccuracies.
"""

response = m.summarize(p)
print(response)
